{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 1 Shopper Prediction\n",
    "Maxwell Warren - msw8gh\n",
    "INFOTC 3040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules / dependicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data from shopping.xlsx using pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    return pd.read_excel(filename)\n",
    "\n",
    "rawShoppingData = loadData(\"shopping.xlsx\")\n",
    "\n",
    "rawTestingData = loadData(\"unseen.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data (converting strings / bools into numerical representation)\n",
    "We are going to create an object to convert string options into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringConverter = {\n",
    "    \"Month\": {\n",
    "        \"Jan\": 1,\n",
    "        \"Feb\": 2,\n",
    "        \"Mar\": 3,\n",
    "        \"Apr\": 4,\n",
    "        \"May\": 5,\n",
    "        \"June\": 6,\n",
    "        \"Jul\": 7,\n",
    "        \"Aug\": 8,\n",
    "        \"Sep\": 9,\n",
    "        \"Oct\": 10,\n",
    "        \"Nov\": 11,\n",
    "        \"Dec\": 12\n",
    "    },\n",
    "\n",
    "    \"VisitorType\": {\n",
    "        \"New_Visitor\": 2,\n",
    "        \"Returning_Visitor\": 1,\n",
    "        \"Other\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "def cleanData(data):\n",
    "    data['Month'] = data['Month'].map(stringConverter['Month']) \n",
    "    data['VisitorType'] = data['VisitorType'].map(stringConverter['VisitorType'])\n",
    "\n",
    "    data['Weekend'] = data['Weekend'].astype(int)\n",
    "    data['Revenue'] = data['Revenue'].astype(int)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "cleanShoppingData = cleanData(rawShoppingData)\n",
    "cleanTestingData = cleanData(rawTestingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split the data into X and y, where X contains training examples and y contains the outcome\n",
    "X will be 5000 x 17 and y will be 5000 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "X_train = cleanShoppingData.iloc[:, :17].to_numpy()\n",
    "y_train = cleanShoppingData[\"Revenue\"].to_numpy()\n",
    "\n",
    "#testing data\n",
    "X_test = cleanTestingData.iloc[:, :17].to_numpy()\n",
    "y_test = cleanTestingData[\"Revenue\"].to_numpy()\n",
    "\n",
    "##X[row][col], y[row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project descriptions says we need to test 3 different feature scaling methods, MinMax, Mean, and Z-Score\n",
    "\n",
    "Since there is alot of shared code / formulas, I will combine all 3 methods into one parent function with a parameter to control the type of scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data is the 5000 x 17 array\n",
    "def scaleData(data, sType):\n",
    "    scaledData = np.zeros(data.shape) #create empty array to put new values in\n",
    "\n",
    "    m, n = data.shape #m = 5000 n = 17\n",
    "    \n",
    "    for c in range(n):\n",
    "        x = data[:, c] #x is the feature column at index c in data. 5000 x 1\n",
    "\n",
    "        x_min = np.min(x)\n",
    "        x_max = np.max(x)\n",
    "        x_mean = np.mean(x)\n",
    "        x_std = np.std(x)\n",
    "\n",
    "        for r in range(m):\n",
    "            x_curr = x[r] #current value we are working with\n",
    "            x_scaled = 0\n",
    "\n",
    "            if sType == \"Minmax\":\n",
    "                x_scaled = (x_curr - x_min) / (x_max - x_min)\n",
    "            elif sType == \"Mean\":\n",
    "                x_scaled = (x_curr - x_mean) / (x_max - x_min)\n",
    "            elif sType == \"Z-Score\":\n",
    "                x_scaled = (x_curr - x_mean) / x_std\n",
    "\n",
    "            scaledData[r, c] = x_scaled #add normalized value to correct spot in new dataset\n",
    "    \n",
    "    return scaledData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I am going to define the sigmoid function aswell as initialize all weights for the features to 0. Theta is often used to represent the weights.\n",
    "I am also going to add a function that adds the bias column to a dataset, since this will be useful for when we are done feature scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def init_weights(n):\n",
    "    #n = number of features  = 18 because we have called the add bias term before this function is called\n",
    "    theta = np.zeros(n) \n",
    "    return theta\n",
    "\n",
    "def add_bias_column(X):\n",
    "    X_bias = np.c_[np.ones(X.shape[0]), X]  #adding column of ones to start of training data\n",
    "    return X_bias\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement the logistic cost function next aswell as a cost  function with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y, theta):\n",
    "    m = len(y)  \n",
    "    z = np.dot(X, theta)  \n",
    "    h = sigmoid(z)  \n",
    "\n",
    "    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "\n",
    "    return cost\n",
    "\n",
    "def cost_function_with_L2(X, y, theta, lambda_):\n",
    "    m = len(y)  \n",
    "    z = np.dot(X, theta)  \n",
    "    h = sigmoid(z)  \n",
    "    \n",
    "    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    regularization_term = (lambda_ / (2*m)) * np.sum(np.square(theta[1:]))  \n",
    "    \n",
    "    total_cost = cost + regularization_term\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to use gradient descent to optimize the weights to overall minimize the cost function. We will define the function below for each of the cost variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    m = len(y)  # number of training examples\n",
    "\n",
    "    for i in range(iterations):\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        gradient = (1/m) * np.dot(X.T, (h - y))  # gradient of the cost function\n",
    "        theta -= learning_rate * gradient  # update weights\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "def gradient_descent_with_L2(X, y, theta, learning_rate, iterations, lambda_):\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        gradient = (1/m) * np.dot(X.T, (h - y))\n",
    "        theta[1:] -= learning_rate * ((1/m) * np.dot(X[:, 1:].T, (h - y)) + (lambda_ / m) * theta[1:])  \n",
    "        \n",
    "        theta[0] -= learning_rate * gradient[0]\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement prediction and accuracy functions for model testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, theta):\n",
    "    z = np.dot(X_test, theta)\n",
    "    probabilities = sigmoid(z)\n",
    "    return [1 if p >= 0.5 else 0 for p in probabilities]\n",
    "\n",
    "def getAccuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a master function to train, test, and log on all scaling types and finding the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all scaling types WITHOUT any regularization.\n",
      "\n",
      "Testing model with Minmax scaling\n",
      "Accuracy with Minmax scaling: 84.8%\n",
      "\n",
      "Testing model with Mean scaling\n",
      "Accuracy with Mean scaling: 84.8%\n",
      "\n",
      "Testing model with Z-Score scaling\n",
      "Accuracy with Z-Score scaling: 86.6%\n",
      "\n",
      "Z-Score has the best accuracy. We are going to retrain the model using L2 regularization with this scaling method, per the instructions.\n",
      "\n",
      "Model Accuracy with L2 Regularization using Z-Score scaling: 86.6%\n"
     ]
    }
   ],
   "source": [
    "def test_models(sTypes):\n",
    "    print(\"Testing all scaling types WITHOUT any regularization.\\n\")\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_scaling_method = \"\"\n",
    "\n",
    "    for sType in sTypes:\n",
    "        print(\"Testing model with\", sType, \"scaling\")\n",
    "\n",
    "        # Scale the training and test data\n",
    "        X_train_scaled = scaleData(X_train, sType)  # Scale data before adding bias column\n",
    "        X_train_bias = add_bias_column(X_train_scaled)  # Training data with added bias column\n",
    "\n",
    "        X_test_scaled = scaleData(X_test, sType)  # Scale the test data\n",
    "        X_test_bias = add_bias_column(X_test_scaled)  # Test data with added bias column\n",
    "\n",
    "        # Initialize weights (all zeros)\n",
    "        theta_initial = init_weights(X_train_bias.shape[1])\n",
    "\n",
    "        # Train the model using gradient descent without regularization\n",
    "        optimized_theta = gradient_descent(X_train_bias, y_train, theta_initial, 0.01, 5000)\n",
    "\n",
    "        y_pred_test = predict(X_test_bias, optimized_theta)\n",
    "\n",
    "        test_accuracy = getAccuracy(y_test, y_pred_test)\n",
    "\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            best_scaling_method = sType\n",
    "\n",
    "        print(f\"Accuracy with {sType} scaling: {test_accuracy}%\\n\")\n",
    "\n",
    "    # Output the best scaling method\n",
    "    print(f\"{best_scaling_method} has the best accuracy. We are going to retrain the model using L2 regularization with this scaling method, per the instructions.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## Retraining with the most effective scaling method (add L2 Regularization)\n",
    "\n",
    "    X_train_scaled = scaleData(X_train, best_scaling_method)\n",
    "    X_train_bias = add_bias_column(X_train_scaled)\n",
    "\n",
    "    X_test_scaled = scaleData(X_test, best_scaling_method)\n",
    "    X_test_bias = add_bias_column(X_test_scaled)\n",
    "\n",
    "    theta_initial = init_weights(X_train_bias.shape[1])\n",
    "\n",
    "    optimized_theta_L2 = gradient_descent_with_L2(X_train_bias, y_train, theta_initial, 0.01, 5000, 0.05) #0.05 is the lamba for L2\n",
    "\n",
    "    y_pred_test_L2 = predict(X_test_bias, optimized_theta_L2)\n",
    "\n",
    "    test_accuracy_L2 = getAccuracy(y_test, y_pred_test_L2)\n",
    "\n",
    "    print(f\"Model Accuracy with L2 Regularization using {best_scaling_method} scaling: {test_accuracy_L2}%\")\n",
    "\n",
    "\n",
    "test_models([\"Minmax\", \"Mean\", \"Z-Score\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
